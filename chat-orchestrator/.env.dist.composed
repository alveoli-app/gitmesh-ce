# Chat Orchestrator - Distribution Configuration (Docker Compose)
# This file is used when running with docker-compose
# Default values for containerized deployment

# =============================================================================
# LLM Provider Selection
# =============================================================================
AI_PROVIDER=auto

# =============================================================================
# Ollama (Local LLM - Default/Recommended)
# =============================================================================
OLLAMA_ENABLED=true
OLLAMA_URL=http://host.docker.internal:11434
OLLAMA_MODEL=llama3.2:3b

# =============================================================================
# Cloud LLM Providers (Optional - Set via docker-compose.override.yaml)
# =============================================================================
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o

ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

GOOGLE_AI_API_KEY=
GOOGLE_AI_MODEL=gemini-2.0-flash-exp

GROQ_API_KEY=
GROQ_MODEL=llama-3.3-70b-versatile

TOGETHER_API_KEY=
TOGETHER_MODEL=meta-llama/Llama-3.3-70B-Instruct-Turbo

# =============================================================================
# Backend Connection
# =============================================================================
BACKEND_URL=http://backend:8081
SERVICE_TOKEN=dev-token

# =============================================================================
# Service Configuration
# =============================================================================
PORT=8001
DEBUG=false
LOG_LEVEL=INFO

# Agent settings
MAX_TOKENS=4096
AGENT_TIMEOUT=120
MAX_ITERATIONS=10

# CrewAI storage
CREWAI_STORAGE_DIR=./storage/crewai

# Rate limiting
INSIGHT_COOLDOWN_HOURS=4
MAX_INSIGHTS_PER_DAY=3

# CORS
CORS_ORIGINS=["http://localhost:8080","http://localhost:3000"]
